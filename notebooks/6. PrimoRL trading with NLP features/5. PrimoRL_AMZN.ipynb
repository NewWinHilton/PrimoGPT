{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FinRL and stable_baselines3 for machine learning and trading\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_primo_trading.env_primorl import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Configuration files and helper functions from FinRL\n",
    "from finrl.config import INDICATORS, FUNDAMENTAL_INDICATORS\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "# Enabling chart display within Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "TRAINED_MODEL_DIR = 'trained_models_primorl'\n",
    "RESULTS_DIR = 'results_primorl'\n",
    "DATA_DIR = 'data_primorl'\n",
    "\n",
    "# Checking and creating directories\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, DATA_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a class called YahooDownloader that uses yfinance to fetch data from Yahoo Finance.\n",
    "\n",
    "In the YahooDownloader of FinRL, we modified the data frame into a format suitable for further data processing. We use a custom closing price instead of the regular closing price and add a column that represents the day of the week (0-4 corresponding to Monday-Friday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and trading intervals\n",
    "TRAIN_START_DATE = '2022-04-01'\n",
    "TRAIN_END_DATE = '2024-07-31'\n",
    "TRADE_START_DATE = '2024-08-01'\n",
    "TRADE_END_DATE = '2025-02-28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Učitavamo podatke iz CSV datoteke koja sadrži generirane značajke od strane GPT-4 modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Adj Close Price</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Bin Label</th>\n",
       "      <th>News Relevance</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Price Impact Potential</th>\n",
       "      <th>Trend Direction</th>\n",
       "      <th>Earnings Impact</th>\n",
       "      <th>Investor Confidence</th>\n",
       "      <th>Risk Profile Change</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>U3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n                [COMPANY BASICS]\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>D3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n                [COMPANY BASICS]\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>-0.032300</td>\n",
       "      <td>D4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n                [COMPANY BASICS]\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>-0.006119</td>\n",
       "      <td>D1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n                [COMPANY BASICS]\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-08</td>\n",
       "      <td>154.460495</td>\n",
       "      <td>-0.021067</td>\n",
       "      <td>D3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n                [COMPANY BASICS]\\n          ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  Adj Close Price   Returns Bin Label  News Relevance  Sentiment  \\\n",
       "0  2022-04-04       168.346497  0.029264        U3               0          0   \n",
       "1  2022-04-05       164.054993 -0.025492        D3               2          1   \n",
       "2  2022-04-06       158.755997 -0.032300        D4               2          1   \n",
       "3  2022-04-07       157.784500 -0.006119        D1               2          0   \n",
       "4  2022-04-08       154.460495 -0.021067        D3               0          0   \n",
       "\n",
       "   Price Impact Potential  Trend Direction  Earnings Impact  \\\n",
       "0                       0                0                0   \n",
       "1                       1                1                1   \n",
       "2                       1                1                1   \n",
       "3                      -1                0                0   \n",
       "4                       0                0                0   \n",
       "\n",
       "   Investor Confidence  Risk Profile Change  \\\n",
       "0                    0                    0   \n",
       "1                    2                    0   \n",
       "2                    2                    0   \n",
       "3                    0                    0   \n",
       "4                    0                    0   \n",
       "\n",
       "                                              Prompt  \n",
       "0  \\n                [COMPANY BASICS]\\n          ...  \n",
       "1  \\n                [COMPANY BASICS]\\n          ...  \n",
       "2  \\n                [COMPANY BASICS]\\n          ...  \n",
       "3  \\n                [COMPANY BASICS]\\n          ...  \n",
       "4  \\n                [COMPANY BASICS]\\n          ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_predictions = pd.read_csv('data/AMZN_data.csv')\n",
    "\n",
    "# Renaming the column 'Date' to 'date'\n",
    "gpt_predictions = gpt_predictions.rename(columns={'Date': 'date'})\n",
    "\n",
    "gpt_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (729, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetching all data\n",
    "df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                         end_date = TRADE_END_DATE,\n",
    "                         ticker_list = ['AMZN']).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>165.826996</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>57090000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.394501</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>49882000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>168.110504</td>\n",
       "      <td>167.741501</td>\n",
       "      <td>53728000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>162.199997</td>\n",
       "      <td>161.650497</td>\n",
       "      <td>79056000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>160.078995</td>\n",
       "      <td>158.399994</td>\n",
       "      <td>68136000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        open        high         low       close    volume   tic  \\\n",
       "0  2022-04-01  163.559998  163.559998  165.826996  164.149506  57090000  AMZN   \n",
       "1  2022-04-04  168.346497  168.346497  168.394501  164.125000  49882000  AMZN   \n",
       "2  2022-04-05  164.054993  164.054993  168.110504  167.741501  53728000  AMZN   \n",
       "3  2022-04-06  158.755997  158.755997  162.199997  161.650497  79056000  AMZN   \n",
       "4  2022-04-07  157.784500  157.784500  160.078995  158.399994  68136000  AMZN   \n",
       "\n",
       "   day  \n",
       "0    4  \n",
       "1    0  \n",
       "2    1  \n",
       "3    2  \n",
       "4    3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the defined class FeatureEngineer to process data and add technical indicators.\n",
    "\n",
    "The use_vix option includes the VIX index, known as the \"fear index,\" which represents the expected volatility of the stock market based on S&P 500 options. The VIX helps the model understand market uncertainty, with higher values indicating greater uncertainty.\n",
    "\n",
    "The use_turbulence option includes the turbulence index, which measures unusual market fluctuations and serves as an indicator of risk and market shocks. It is useful for identifying periods of high volatility, helping the model to adapt to financial crises.\n",
    "\n",
    "Overall, a quite handy class that contains useful methods for processing financial data and can be applied to numerous other purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Shape of DataFrame:  (728, 8)\n",
      "Successfully added vix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_vix=True,\n",
    "                     use_turbulence=False,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique stock tickers from the 'tic' column of the 'processed' DataFrame.\n",
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "\n",
    "# Create a list of dates between the earliest and latest dates in the 'processed' DataFrame, converting them to strings.\n",
    "list_date = list(pd.date_range(processed['date'].min(), processed['date'].max()).astype(str))\n",
    "\n",
    "# Create combinations of all dates and stock tickers using the Cartesian product.\n",
    "combination = list(itertools.product(list_date, list_ticker))\n",
    "\n",
    "# Create a new DataFrame 'processed_full' with columns \"date\" and \"tic\", containing all combinations of dates and stock tickers.\n",
    "# Merge this DataFrame with the original 'processed' DataFrame based on the \"date\" and \"tic\" columns, using a 'left' join.\n",
    "processed_full = pd.DataFrame(combination, columns=[\"date\", \"tic\"]).merge(processed, on=[\"date\", \"tic\"], how=\"left\")\n",
    "\n",
    "# Filter the 'processed_full' DataFrame to contain only those rows whose dates are present in the original 'processed' DataFrame.\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "\n",
    "# Sort the 'processed_full' DataFrame by date and stock ticker.\n",
    "processed_full = processed_full.sort_values(['date', 'tic'])\n",
    "\n",
    "# Replace all missing values (NaN) with 0 in the 'processed_full' DataFrame.\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>165.826996</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>57090000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.171909</td>\n",
       "      <td>164.102597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>20.620001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.394501</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>49882000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>164.171909</td>\n",
       "      <td>164.102597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>164.137253</td>\n",
       "      <td>164.137253</td>\n",
       "      <td>20.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>168.110504</td>\n",
       "      <td>167.741501</td>\n",
       "      <td>53728000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111447</td>\n",
       "      <td>169.500568</td>\n",
       "      <td>161.176770</td>\n",
       "      <td>99.349244</td>\n",
       "      <td>39.500002</td>\n",
       "      <td>88.434139</td>\n",
       "      <td>165.338669</td>\n",
       "      <td>165.338669</td>\n",
       "      <td>18.790001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>162.199997</td>\n",
       "      <td>161.650497</td>\n",
       "      <td>79056000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>169.431625</td>\n",
       "      <td>159.401627</td>\n",
       "      <td>36.378815</td>\n",
       "      <td>-125.835733</td>\n",
       "      <td>16.066422</td>\n",
       "      <td>164.416626</td>\n",
       "      <td>164.416626</td>\n",
       "      <td>21.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>160.078995</td>\n",
       "      <td>158.399994</td>\n",
       "      <td>68136000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.281638</td>\n",
       "      <td>170.128684</td>\n",
       "      <td>156.297915</td>\n",
       "      <td>26.949055</td>\n",
       "      <td>-106.928906</td>\n",
       "      <td>30.395985</td>\n",
       "      <td>163.213300</td>\n",
       "      <td>163.213300</td>\n",
       "      <td>21.969999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tic        open        high         low       close  \\\n",
       "0  2022-04-01  AMZN  163.559998  163.559998  165.826996  164.149506   \n",
       "3  2022-04-04  AMZN  168.346497  168.346497  168.394501  164.125000   \n",
       "4  2022-04-05  AMZN  164.054993  164.054993  168.110504  167.741501   \n",
       "5  2022-04-06  AMZN  158.755997  158.755997  162.199997  161.650497   \n",
       "6  2022-04-07  AMZN  157.784500  157.784500  160.078995  158.399994   \n",
       "\n",
       "       volume  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0  57090000.0  4.0  0.000000  164.171909  164.102597   0.000000   66.666667   \n",
       "3  49882000.0  0.0 -0.000550  164.171909  164.102597   0.000000   66.666667   \n",
       "4  53728000.0  1.0  0.111447  169.500568  161.176770  99.349244   39.500002   \n",
       "5  79056000.0  2.0 -0.060259  169.431625  159.401627  36.378815 -125.835733   \n",
       "6  68136000.0  3.0 -0.281638  170.128684  156.297915  26.949055 -106.928906   \n",
       "\n",
       "        dx_30  close_30_sma  close_60_sma        vix  \n",
       "0  100.000000    164.149506    164.149506  20.620001  \n",
       "3  100.000000    164.137253    164.137253  20.750000  \n",
       "4   88.434139    165.338669    165.338669  18.790001  \n",
       "5   16.066422    164.416626    164.416626  21.270000  \n",
       "6   30.395985    163.213300    163.213300  21.969999  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We merge these fetched data with the data from the CSV file containing the generated features by the PrimoGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging by date\n",
    "processed_full = processed_full.merge(gpt_predictions, on='date', how='left')\n",
    "processed_full = processed_full.fillna(0)\n",
    "\n",
    "# Removing unnecessary columns\n",
    "columns_to_drop = ['Adj Close Price', 'Returns', 'Bin Label', 'Prompt']\n",
    "processed_full = processed_full.drop(columns=columns_to_drop)\n",
    "\n",
    "# Manually renaming specific columns\n",
    "column_mapping = {\n",
    "    'News Relevance': 'news_relevance',\n",
    "    'Sentiment': 'sentiment',\n",
    "    'Price Impact Potential': 'price_impact_potential',\n",
    "    'Trend Direction': 'trend_direction',\n",
    "    'Earnings Impact': 'earnings_impact',\n",
    "    'Investor Confidence': 'investor_confidence',\n",
    "    'Risk Profile Change': 'risk_profile_change'\n",
    "}\n",
    "\n",
    "# Renaming columns\n",
    "processed_full = processed_full.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>news_relevance</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>price_impact_potential</th>\n",
       "      <th>trend_direction</th>\n",
       "      <th>earnings_impact</th>\n",
       "      <th>investor_confidence</th>\n",
       "      <th>risk_profile_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>163.559998</td>\n",
       "      <td>165.826996</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>57090000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.171909</td>\n",
       "      <td>...</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>164.149506</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.346497</td>\n",
       "      <td>168.394501</td>\n",
       "      <td>164.125000</td>\n",
       "      <td>49882000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>164.171909</td>\n",
       "      <td>...</td>\n",
       "      <td>164.137253</td>\n",
       "      <td>164.137253</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>164.054993</td>\n",
       "      <td>168.110504</td>\n",
       "      <td>167.741501</td>\n",
       "      <td>53728000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111447</td>\n",
       "      <td>169.500568</td>\n",
       "      <td>...</td>\n",
       "      <td>165.338669</td>\n",
       "      <td>165.338669</td>\n",
       "      <td>18.790001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>158.755997</td>\n",
       "      <td>162.199997</td>\n",
       "      <td>161.650497</td>\n",
       "      <td>79056000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>169.431625</td>\n",
       "      <td>...</td>\n",
       "      <td>164.416626</td>\n",
       "      <td>164.416626</td>\n",
       "      <td>21.270000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>157.784500</td>\n",
       "      <td>160.078995</td>\n",
       "      <td>158.399994</td>\n",
       "      <td>68136000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.281638</td>\n",
       "      <td>170.128684</td>\n",
       "      <td>...</td>\n",
       "      <td>163.213300</td>\n",
       "      <td>163.213300</td>\n",
       "      <td>21.969999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tic        open        high         low       close  \\\n",
       "0  2022-04-01  AMZN  163.559998  163.559998  165.826996  164.149506   \n",
       "1  2022-04-04  AMZN  168.346497  168.346497  168.394501  164.125000   \n",
       "2  2022-04-05  AMZN  164.054993  164.054993  168.110504  167.741501   \n",
       "3  2022-04-06  AMZN  158.755997  158.755997  162.199997  161.650497   \n",
       "4  2022-04-07  AMZN  157.784500  157.784500  160.078995  158.399994   \n",
       "\n",
       "       volume  day      macd     boll_ub  ...  close_30_sma  close_60_sma  \\\n",
       "0  57090000.0  4.0  0.000000  164.171909  ...    164.149506    164.149506   \n",
       "1  49882000.0  0.0 -0.000550  164.171909  ...    164.137253    164.137253   \n",
       "2  53728000.0  1.0  0.111447  169.500568  ...    165.338669    165.338669   \n",
       "3  79056000.0  2.0 -0.060259  169.431625  ...    164.416626    164.416626   \n",
       "4  68136000.0  3.0 -0.281638  170.128684  ...    163.213300    163.213300   \n",
       "\n",
       "         vix  news_relevance  sentiment  price_impact_potential  \\\n",
       "0  20.620001             0.0        0.0                     0.0   \n",
       "1  20.750000             0.0        0.0                     0.0   \n",
       "2  18.790001             2.0        1.0                     1.0   \n",
       "3  21.270000             2.0        1.0                     1.0   \n",
       "4  21.969999             2.0        0.0                    -1.0   \n",
       "\n",
       "   trend_direction  earnings_impact  investor_confidence  risk_profile_change  \n",
       "0              0.0              0.0                  0.0                  0.0  \n",
       "1              0.0              0.0                  0.0                  0.0  \n",
       "2              1.0              1.0                  2.0                  0.0  \n",
       "3              1.0              1.0                  2.0                  0.0  \n",
       "4              0.0              0.0                  0.0                  0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "# Preparing the training and trade set according to the defined dates\n",
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data\n",
    "train.to_csv(DATA_DIR + '/train_data.csv')\n",
    "trade.to_csv(DATA_DIR + '/trade_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stock_dimension represents the number of unique stocks in the dataset. This value is used to determine how many different stocks the model can trade.\n",
    "\n",
    "state_space denotes the total size of the state space that the model uses for decision-making. The state space includes current stock data (such as prices and technical indicators) and information about the current portfolio (such as the number of shares owned). The size of the state space is determined by a formula that takes into account the number of stocks and the number of technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 18\n"
     ]
    }
   ],
   "source": [
    "# Calculating the dimension (number of unique stocks) from the 'train' DataFrame.\n",
    "stock_dimension = len(train.tic.unique())\n",
    "\n",
    "# Calculating the state size based on the dimension and the number of technical indicators.\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension + len(FUNDAMENTAL_INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of buy and sell costs for each stock.\n",
    "buy_cost_list = sell_cost_list = [0] * stock_dimension\n",
    "\n",
    "# Initializes the number of shares for each stock to 0.\n",
    "num_stock_shares = [0] * stock_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e_train_gym is an instance of the StockTradingEnv class, which represents the trading environment. This environment uses the data and parameters defined to simulate the stock market. The model learns how to trade within this environment, trying to maximize the total reward (profit) through a series of trading decisions.\n",
    "\n",
    "env_train is obtained by calling the get_sb_env() method on the e_train_gym instance. This method returns an environment compatible with the Stable Baselines 3 library, which is used for implementing reinforcement learning algorithms. The second return element, which is ignored here (_), can be used for additional information or functionalities provided by the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "# Define arguments for the trading environment.\n",
    "env_kwargs = {\n",
    "    \"hmax\": 1000,  # Maximum number of shares that can be bought or sold in a single transaction.\n",
    "    \"initial_amount\": 100000,  # Initial amount of capital.\n",
    "    \"num_stock_shares\": num_stock_shares,  # Initial number of shares for each stock in the portfolio, initially set to 0.\n",
    "    \"buy_cost_pct\": buy_cost_list,  # Percentages of costs for buying and selling stocks, simulating actual transaction costs.\n",
    "    \"sell_cost_pct\": sell_cost_list,  # Percentages of costs for buying and selling stocks, simulating actual transaction costs.\n",
    "    \"state_space\": state_space,  # Previously defined sizes that affect the structure of the environment.\n",
    "    \"stock_dim\": stock_dimension,  # Previously defined sizes that affect the structure of the environment.\n",
    "    \"tech_indicator_list\": INDICATORS,  # List of technical indicators to be used for market state analysis.\n",
    "    \"fundamental_indicator_list\": FUNDAMENTAL_INDICATORS,  # List of fundamental indicators to be used for market state analysis.\n",
    "    \"action_space\": stock_dimension,  # Size of the action space, i.e., the number of different actions the model can take.\n",
    "    \"reward_scaling\": 1e-2,  # Scaling factor for the reward, used to adjust the reward size to facilitate learning.\n",
    "    \"verbose\": 0\n",
    "}\n",
    "\n",
    "# Creates the training environment.\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
    "\n",
    "# Retrieves the environment suitable for Stable Baselines and an unused object.\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training DRL Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the deep reinforcement learning (DRL) algorithms from the Stable Baselines 3 library. This is a library that has implemented popular DRL algorithms using PyTorch.\n",
    "\n",
    "As an alternative, the ElegantRL and Ray RLlib libraries can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 5 different DRL agents (A2C, DDPG, PPO, TD3, SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the appropriate values to 'True' for the algorithms you want to use\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 1: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# A new instance of the DRL agent is created with the given environment (it is not necessary to do this again, but for easier instructions, it remains)\n",
    "agent = DRLAgent(env = env_train)\n",
    "# Retrieves the A2C model using the get_model method\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # Setting up the logger to monitor and record information during training\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  # Configures a new logger that will print information to standard output, log to a CSV file, and TensorBoard\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Sets the new logger for the A2C model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the A2C model with the defined total number of time steps, only if if_using_a2c is set to True.\n",
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                tb_log_name='a2c',\n",
    "                                total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained A2C model to the defined directory, only if if_using_a2c is set to True.\n",
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results_primorl/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "       \"n_steps\": 2048,\n",
    "       \"ent_coef\": 0.01,\n",
    "       \"learning_rate\": 0.00025,\n",
    "       \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 2640         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 0            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0004247593 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2131         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045777457 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.24        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00441      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    reward               | -0.030883359 |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.0239       |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 10\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 58914.80\n",
      "Total reward: -41085.20\n",
      "Total cost: 0.00\n",
      "Total trades: 349\n",
      "Sharpe: -0.604\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2132          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0038479678  |\n",
      "|    clip_fraction        | 0.0293        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.0896       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00819      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0029       |\n",
      "|    reward               | -0.0008446116 |\n",
      "|    std                  | 0.993         |\n",
      "|    value_loss           | 0.0131        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2143         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026451019 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0132       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00624     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    reward               | -0.016916575 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0157       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2155          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00443628    |\n",
      "|    clip_fraction        | 0.0228        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.00986      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0146       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00243      |\n",
      "|    reward               | -0.0010376421 |\n",
      "|    std                  | 0.979         |\n",
      "|    value_loss           | 0.00971       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 20\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 119988.24\n",
      "Total reward: 19988.24\n",
      "Total cost: 0.00\n",
      "Total trades: 381\n",
      "Sharpe: 0.447\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2161         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016444    |\n",
      "|    clip_fraction        | 0.00479      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0908       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    reward               | -0.018233474 |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 0.00799      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2140         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039644283 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0172      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00326     |\n",
      "|    reward               | 0.0012543445 |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 0.00677      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2147         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041313604 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    reward               | -0.012909071 |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.0071       |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 30\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 114267.06\n",
      "Total reward: 14267.06\n",
      "Total cost: 0.00\n",
      "Total trades: 397\n",
      "Sharpe: 0.350\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2150          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003492592   |\n",
      "|    clip_fraction        | 0.0358        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -0.0224       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0125       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00296      |\n",
      "|    reward               | -0.0011115837 |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 0.00458       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2149        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004072047 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    reward               | -0.00376629 |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2143          |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0020263433  |\n",
      "|    clip_fraction        | 0.0135        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.118         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0139       |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.00227      |\n",
      "|    reward               | -0.0011201616 |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 0.00481       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 40\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 89267.53\n",
      "Total reward: -10732.47\n",
      "Total cost: 0.00\n",
      "Total trades: 338\n",
      "Sharpe: -0.014\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2131         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055242935 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00547     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    reward               | -0.010511109 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00462      |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2136           |\n",
      "|    iterations           | 13             |\n",
      "|    time_elapsed         | 12             |\n",
      "|    total_timesteps      | 26624          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0028096787   |\n",
      "|    clip_fraction        | 0.0203         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.263          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0195        |\n",
      "|    n_updates            | 120            |\n",
      "|    policy_gradient_loss | -0.00311       |\n",
      "|    reward               | -0.00024605467 |\n",
      "|    std                  | 1.01           |\n",
      "|    value_loss           | 0.00601        |\n",
      "--------------------------------------------\n",
      "Day: 583, episode: 50\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 121041.56\n",
      "Total reward: 21041.56\n",
      "Total cost: 0.00\n",
      "Total trades: 393\n",
      "Sharpe: 0.440\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2140         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001714193  |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0949       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0187      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    reward               | -0.007394658 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00477      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2143          |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0048765666  |\n",
      "|    clip_fraction        | 0.0181        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.23          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.02         |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00272      |\n",
      "|    reward               | -0.0013248926 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0053        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2147         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002384848  |\n",
      "|    clip_fraction        | 0.00981      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    reward               | -0.007246066 |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 60\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 131709.04\n",
      "Total reward: 31709.04\n",
      "Total cost: 0.00\n",
      "Total trades: 382\n",
      "Sharpe: 0.584\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2151         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024431671 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0197      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    reward               | 0.0039808555 |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2153          |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0026012838  |\n",
      "|    clip_fraction        | 0.0147        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0891        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00911      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.0017       |\n",
      "|    reward               | -0.0033827992 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 0.00359       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2156          |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0029645236  |\n",
      "|    clip_fraction        | 0.00952       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.175         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0108       |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.00292      |\n",
      "|    reward               | -0.0012333861 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 0.00535       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 70\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 112549.10\n",
      "Total reward: 12549.10\n",
      "Total cost: 0.00\n",
      "Total trades: 369\n",
      "Sharpe: 0.334\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2158        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002349954 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    reward               | 0.001209672 |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.0061      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2159          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003686239   |\n",
      "|    clip_fraction        | 0.034         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0726        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0171       |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00338      |\n",
      "|    reward               | -0.0017005964 |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 0.0048        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2159         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003391801  |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0574       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    reward               | -0.003165704 |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 80\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 115025.17\n",
      "Total reward: 15025.17\n",
      "Total cost: 0.00\n",
      "Total trades: 359\n",
      "Sharpe: 0.355\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2160        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005740938 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0684      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0077     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    reward               | -0.00307888 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2161         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043807486 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0273      |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    reward               | 0.0027130544 |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00839      |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2159           |\n",
      "|    iterations           | 25             |\n",
      "|    time_elapsed         | 23             |\n",
      "|    total_timesteps      | 51200          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0035830908   |\n",
      "|    clip_fraction        | 0.0236         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0246         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0182        |\n",
      "|    n_updates            | 240            |\n",
      "|    policy_gradient_loss | -0.0033        |\n",
      "|    reward               | -0.00029473164 |\n",
      "|    std                  | 1.01           |\n",
      "|    value_loss           | 0.00419        |\n",
      "--------------------------------------------\n",
      "Day: 583, episode: 90\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 115918.85\n",
      "Total reward: 15918.85\n",
      "Total cost: 0.00\n",
      "Total trades: 393\n",
      "Sharpe: 0.379\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2159          |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0040952666  |\n",
      "|    clip_fraction        | 0.0224        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.0652        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0115       |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.0032       |\n",
      "|    reward               | -0.0006814786 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00309       |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2160           |\n",
      "|    iterations           | 27             |\n",
      "|    time_elapsed         | 25             |\n",
      "|    total_timesteps      | 55296          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0049039186   |\n",
      "|    clip_fraction        | 0.0456         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.43          |\n",
      "|    explained_variance   | 0.0641         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0278        |\n",
      "|    n_updates            | 260            |\n",
      "|    policy_gradient_loss | -0.0036        |\n",
      "|    reward               | -0.00036697066 |\n",
      "|    std                  | 1.01           |\n",
      "|    value_loss           | 0.00336        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2160          |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003831368   |\n",
      "|    clip_fraction        | 0.0259        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.0176        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0299       |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.00357      |\n",
      "|    reward               | 0.00069604744 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00349       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 100\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 189554.03\n",
      "Total reward: 89554.03\n",
      "Total cost: 0.00\n",
      "Total trades: 394\n",
      "Sharpe: 1.139\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2158         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026183012 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0167      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    reward               | 0.0014520225 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00403      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2153          |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0053535732  |\n",
      "|    clip_fraction        | 0.0355        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.112         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0251       |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00386      |\n",
      "|    reward               | -0.0012851281 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00378       |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2141           |\n",
      "|    iterations           | 31             |\n",
      "|    time_elapsed         | 29             |\n",
      "|    total_timesteps      | 63488          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0044147708   |\n",
      "|    clip_fraction        | 0.0308         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.0832         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0132        |\n",
      "|    n_updates            | 300            |\n",
      "|    policy_gradient_loss | -0.00278       |\n",
      "|    reward               | -0.00031842227 |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 0.00272        |\n",
      "--------------------------------------------\n",
      "Day: 583, episode: 110\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 87883.32\n",
      "Total reward: -12116.68\n",
      "Total cost: 0.00\n",
      "Total trades: 393\n",
      "Sharpe: -0.064\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2141         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053277975 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    reward               | -0.00477917  |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2142          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003907476   |\n",
      "|    clip_fraction        | 0.0413        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.236         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00307      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00407      |\n",
      "|    reward               | 0.00074061635 |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 0.0083        |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 120\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 123428.46\n",
      "Total reward: 23428.46\n",
      "Total cost: 0.00\n",
      "Total trades: 396\n",
      "Sharpe: 0.458\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2142         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025261322 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.284        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    reward               | 0.0039139194 |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2144         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005204603  |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00776     |\n",
      "|    reward               | 0.0028848443 |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 0.00628      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2145          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0057888604  |\n",
      "|    clip_fraction        | 0.0294        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.104         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0142       |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00335      |\n",
      "|    reward               | -0.0022793384 |\n",
      "|    std                  | 0.974         |\n",
      "|    value_loss           | 0.00409       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 130\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 94113.79\n",
      "Total reward: -5886.21\n",
      "Total cost: 0.00\n",
      "Total trades: 382\n",
      "Sharpe: 0.032\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2146          |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003406133   |\n",
      "|    clip_fraction        | 0.0414        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.154         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0182       |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00172      |\n",
      "|    reward               | 0.00025846012 |\n",
      "|    std                  | 0.971         |\n",
      "|    value_loss           | 0.00451       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2136         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039763507 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.08         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0089      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    reward               | -0.003867099 |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2137          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005012142   |\n",
      "|    clip_fraction        | 0.034         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.0695        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0376       |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00423      |\n",
      "|    reward               | 0.00086829916 |\n",
      "|    std                  | 0.974         |\n",
      "|    value_loss           | 0.00341       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 140\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 113190.59\n",
      "Total reward: 13190.59\n",
      "Total cost: 0.00\n",
      "Total trades: 362\n",
      "Sharpe: 0.329\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2138         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052219257 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0216      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00398     |\n",
      "|    reward               | -0.005551492 |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2136          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0045612343  |\n",
      "|    clip_fraction        | 0.0449        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.222         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0303       |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.00522      |\n",
      "|    reward               | -0.0010375296 |\n",
      "|    std                  | 0.972         |\n",
      "|    value_loss           | 0.00765       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2137          |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 40            |\n",
      "|    total_timesteps      | 86016         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0057435855  |\n",
      "|    clip_fraction        | 0.0649        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.165         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0335       |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.00571      |\n",
      "|    reward               | -0.0035813903 |\n",
      "|    std                  | 0.979         |\n",
      "|    value_loss           | 0.00199       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 150\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 132909.48\n",
      "Total reward: 32909.48\n",
      "Total cost: 0.00\n",
      "Total trades: 372\n",
      "Sharpe: 0.589\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2138         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048727994 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00186     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    reward               | 0.0027723932 |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2140         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052960636 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.078        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0328      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00411     |\n",
      "|    reward               | -0.004534238 |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.00259      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2141          |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00401202    |\n",
      "|    clip_fraction        | 0.0347        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.149         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0245       |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.00331      |\n",
      "|    reward               | -0.0020292636 |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 0.00554       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 160\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 70741.07\n",
      "Total reward: -29258.93\n",
      "Total cost: 0.00\n",
      "Total trades: 367\n",
      "Sharpe: -0.433\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2141          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0052175373  |\n",
      "|    clip_fraction        | 0.0519        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.0918        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0161       |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.00394      |\n",
      "|    reward               | -0.0043072775 |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 0.00334       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2142         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00597924   |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.175        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0219      |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00675     |\n",
      "|    reward               | 0.0030359703 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.00479      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2143          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 45            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0044915723  |\n",
      "|    clip_fraction        | 0.054         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.112         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0184       |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.00328      |\n",
      "|    reward               | -0.0032176024 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00305       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 170\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 106192.08\n",
      "Total reward: 6192.08\n",
      "Total cost: 0.00\n",
      "Total trades: 376\n",
      "Sharpe: 0.236\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2144         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006709556  |\n",
      "|    clip_fraction        | 0.0463       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0201      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    reward               | 0.0026947125 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2145          |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0044298535  |\n",
      "|    clip_fraction        | 0.0371        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.148         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.02         |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.00311      |\n",
      "|    reward               | -0.0023484412 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00498       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2145         |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077649783 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.203        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0235      |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    reward               | 0.0006737945 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00387      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 180\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 106665.54\n",
      "Total reward: 6665.54\n",
      "Total cost: 0.00\n",
      "Total trades: 371\n",
      "Sharpe: 0.245\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2144         |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005718238  |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0658       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0131      |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    reward               | 0.0010665447 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00258      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2145         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063431226 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0301      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    reward               | 0.0011107745 |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 190\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 145741.97\n",
      "Total reward: 45741.97\n",
      "Total cost: 0.00\n",
      "Total trades: 358\n",
      "Sharpe: 0.678\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2144          |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 51            |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008458532   |\n",
      "|    clip_fraction        | 0.0614        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.232         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0136       |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | -0.00471      |\n",
      "|    reward               | -0.0014320888 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00313       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2143         |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060404735 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00894     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    reward               | 0.0027339607 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2136          |\n",
      "|    iterations           | 56            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 114688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008706999   |\n",
      "|    clip_fraction        | 0.0683        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.156         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0264       |\n",
      "|    n_updates            | 550           |\n",
      "|    policy_gradient_loss | -0.00509      |\n",
      "|    reward               | -0.0035494452 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00216       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 200\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 158749.95\n",
      "Total reward: 58749.95\n",
      "Total cost: 0.00\n",
      "Total trades: 330\n",
      "Sharpe: 0.777\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2134         |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008120591  |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    reward               | 0.0026663463 |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.0052       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2132          |\n",
      "|    iterations           | 58            |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 118784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008483866   |\n",
      "|    clip_fraction        | 0.0791        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00183       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0259       |\n",
      "|    n_updates            | 570           |\n",
      "|    policy_gradient_loss | -0.0043       |\n",
      "|    reward               | -0.0006545825 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00274       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2127         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 120832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052288026 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.289        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0173      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    reward               | 0.0017201724 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0038       |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 210\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 137834.00\n",
      "Total reward: 37834.00\n",
      "Total cost: 0.00\n",
      "Total trades: 330\n",
      "Sharpe: 0.588\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2120          |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 57            |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0066305036  |\n",
      "|    clip_fraction        | 0.0501        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.26          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00459      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.00363      |\n",
      "|    reward               | 0.00078462873 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.00234       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2103         |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056840363 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.295        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00402     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    reward               | 0.002184331  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2098        |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008871079 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    reward               | 0.001353011 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "Day: 583, episode: 220\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 187405.00\n",
      "Total reward: 87405.00\n",
      "Total cost: 0.00\n",
      "Total trades: 251\n",
      "Sharpe: 0.979\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2093          |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 61            |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007238555   |\n",
      "|    clip_fraction        | 0.0664        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.246         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0148       |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.00411      |\n",
      "|    reward               | 0.00020738764 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00513       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2089        |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011329979 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | 0.001511984 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2079          |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 64            |\n",
      "|    total_timesteps      | 133120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008438481   |\n",
      "|    clip_fraction        | 0.065         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.128         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0201       |\n",
      "|    n_updates            | 640           |\n",
      "|    policy_gradient_loss | -0.00479      |\n",
      "|    reward               | 0.00014350112 |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 0.00312       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 230\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 196327.38\n",
      "Total reward: 96327.38\n",
      "Total cost: 0.00\n",
      "Total trades: 259\n",
      "Sharpe: 1.080\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2074         |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004359899  |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.301        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0182      |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    reward               | 0.0019019417 |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2070        |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003282508 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    reward               | 0.002316072 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.00231     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2070          |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0064185984  |\n",
      "|    clip_fraction        | 0.0424        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.307         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0332       |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    reward               | -0.0015510044 |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 0.00229       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 240\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 106329.71\n",
      "Total reward: 6329.71\n",
      "Total cost: 0.00\n",
      "Total trades: 302\n",
      "Sharpe: 0.242\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2071         |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065304907 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0163      |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.000881    |\n",
      "|    reward               | 0.002473255  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2067          |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 143360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0076361233  |\n",
      "|    clip_fraction        | 0.0637        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.157         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.02         |\n",
      "|    n_updates            | 690           |\n",
      "|    policy_gradient_loss | -0.00301      |\n",
      "|    reward               | 0.00025931472 |\n",
      "|    std                  | 0.991         |\n",
      "|    value_loss           | 0.00139       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2056         |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.014690705  |\n",
      "|    clip_fraction        | 0.0796       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0165      |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    reward               | 0.0024098372 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00224      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 250\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 175564.12\n",
      "Total reward: 75564.12\n",
      "Total cost: 0.00\n",
      "Total trades: 255\n",
      "Sharpe: 0.915\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2046         |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077395183 |\n",
      "|    clip_fraction        | 0.0824       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0186      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00537     |\n",
      "|    reward               | 0.0010814434 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2034         |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008906074  |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0353      |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    reward               | 0.0037056822 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 260\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 154407.54\n",
      "Total reward: 54407.54\n",
      "Total cost: 0.00\n",
      "Total trades: 242\n",
      "Sharpe: 0.721\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2026          |\n",
      "|    iterations           | 74            |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 151552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.011096258   |\n",
      "|    clip_fraction        | 0.0795        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.374         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00711      |\n",
      "|    n_updates            | 730           |\n",
      "|    policy_gradient_loss | -0.00553      |\n",
      "|    reward               | -0.0022560675 |\n",
      "|    std                  | 0.99          |\n",
      "|    value_loss           | 0.00258       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2019        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007099893 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00269    |\n",
      "|    reward               | 0.010813143 |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 2018           |\n",
      "|    iterations           | 76             |\n",
      "|    time_elapsed         | 77             |\n",
      "|    total_timesteps      | 155648         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.008129402    |\n",
      "|    clip_fraction        | 0.0949         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.41          |\n",
      "|    explained_variance   | 0.162          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0273        |\n",
      "|    n_updates            | 750            |\n",
      "|    policy_gradient_loss | -0.00636       |\n",
      "|    reward               | -9.2886694e-05 |\n",
      "|    std                  | 0.986          |\n",
      "|    value_loss           | 0.00219        |\n",
      "--------------------------------------------\n",
      "Day: 583, episode: 270\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 113432.26\n",
      "Total reward: 13432.26\n",
      "Total cost: 0.00\n",
      "Total trades: 267\n",
      "Sharpe: 0.332\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2017         |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006968432  |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0225      |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    reward               | -0.004110799 |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 0.0021       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2010          |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 79            |\n",
      "|    total_timesteps      | 159744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00726338    |\n",
      "|    clip_fraction        | 0.0629        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.333         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0156       |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | -0.00425      |\n",
      "|    reward               | 0.00021951641 |\n",
      "|    std                  | 0.979         |\n",
      "|    value_loss           | 0.00372       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2005         |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008229446  |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.334        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    reward               | -0.006475148 |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.00333      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 280\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 121032.15\n",
      "Total reward: 21032.15\n",
      "Total cost: 0.00\n",
      "Total trades: 252\n",
      "Sharpe: 0.424\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2004          |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008424878   |\n",
      "|    clip_fraction        | 0.0706        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.313         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0191       |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.00259      |\n",
      "|    reward               | 0.00059520284 |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 0.00252       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2004         |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009685422  |\n",
      "|    clip_fraction        | 0.0784       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.368        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0165      |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    reward               | -0.012976591 |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.00204      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2001         |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008533029  |\n",
      "|    clip_fraction        | 0.0777       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.332        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00403     |\n",
      "|    reward               | 0.0013707239 |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 290\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 146445.53\n",
      "Total reward: 46445.53\n",
      "Total cost: 0.00\n",
      "Total trades: 172\n",
      "Sharpe: 0.643\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1998        |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010155877 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    reward               | -0.00461394 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1998           |\n",
      "|    iterations           | 84             |\n",
      "|    time_elapsed         | 86             |\n",
      "|    total_timesteps      | 172032         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.008035701    |\n",
      "|    clip_fraction        | 0.0793         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.429          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0102        |\n",
      "|    n_updates            | 830            |\n",
      "|    policy_gradient_loss | -0.00458       |\n",
      "|    reward               | -0.00048369006 |\n",
      "|    std                  | 0.999          |\n",
      "|    value_loss           | 0.0023         |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1995          |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 87            |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010907596   |\n",
      "|    clip_fraction        | 0.0778        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.391         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0228       |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00408      |\n",
      "|    reward               | -0.0068860506 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00205       |\n",
      "-------------------------------------------\n",
      "Day: 583, episode: 300\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 135609.12\n",
      "Total reward: 35609.12\n",
      "Total cost: 0.00\n",
      "Total trades: 206\n",
      "Sharpe: 0.552\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1984        |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009347174 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00268    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00296    |\n",
      "|    reward               | 0.001818232 |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1984          |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 89            |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014557937   |\n",
      "|    clip_fraction        | 0.101         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.476         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0254       |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.00597      |\n",
      "|    reward               | 0.00012458951 |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 0.0021        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1977         |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008713271  |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.508        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    reward               | 0.0005852755 |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 310\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 128474.62\n",
      "Total reward: 28474.62\n",
      "Total cost: 0.00\n",
      "Total trades: 219\n",
      "Sharpe: 0.500\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1978          |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 92            |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010926276   |\n",
      "|    clip_fraction        | 0.0904        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.489         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0163       |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | -0.00671      |\n",
      "|    reward               | -0.0029787945 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.00386       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1975         |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007184459  |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0122      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    reward               | 0.0014870863 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 320\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 142798.47\n",
      "Total reward: 42798.47\n",
      "Total cost: 0.00\n",
      "Total trades: 180\n",
      "Sharpe: 0.617\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1972         |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006320688  |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00188      |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    reward               | -0.003422646 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1970         |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054659727 |\n",
      "|    clip_fraction        | 0.0835       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    reward               | 0.0012557073 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00268      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1969         |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011375618  |\n",
      "|    clip_fraction        | 0.084        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0341      |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    reward               | -0.005256169 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00191      |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 330\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 95134.58\n",
      "Total reward: -4865.42\n",
      "Total cost: 0.00\n",
      "Total trades: 193\n",
      "Sharpe: 0.098\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1967         |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104073305 |\n",
      "|    clip_fraction        | 0.0806       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0398      |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    reward               | 0.0011066777 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00232      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1965        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005756403 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    reward               | 0.005401256 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00284     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1963         |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009994983  |\n",
      "|    clip_fraction        | 0.0904       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.479        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0209      |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    reward               | 0.0019113437 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "Day: 583, episode: 340\n",
      "Begin total asset: 100000.00\n",
      "End total asset: 152811.06\n",
      "Total reward: 52811.06\n",
      "Total cost: 0.00\n",
      "Total trades: 156\n",
      "Sharpe: 0.713\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1962         |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009979418  |\n",
      "|    clip_fraction        | 0.0604       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.55         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0284      |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    reward               | 0.0005300081 |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00168      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_ppo = agent.train_model(model=model_ppo, \n\u001b[32m      2\u001b[39m                              tb_log_name=\u001b[33m'\u001b[39m\u001b[33mppo\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                              total_timesteps=\u001b[32m400000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m if_using_ppo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/Privatno/PrimoGPT/notebooks/6. PrimoRL trading with NLP features/../../finrl/agents/stablebaselines3/models.py:117\u001b[39m, in \u001b[36mDRLAgent.train_model\u001b[39m\u001b[34m(model, tb_log_name, total_timesteps)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mtrain_model\u001b[39m(\n\u001b[32m    115\u001b[39m     model, tb_log_name, total_timesteps=\u001b[32m5000\u001b[39m\n\u001b[32m    116\u001b[39m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     model = model.learn(\n\u001b[32m    118\u001b[39m         total_timesteps=total_timesteps,\n\u001b[32m    119\u001b[39m         tb_log_name=tb_log_name,\n\u001b[32m    120\u001b[39m         callback=TensorboardCallback(),\n\u001b[32m    121\u001b[39m     )\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().learn(\n\u001b[32m    312\u001b[39m         total_timesteps=total_timesteps,\n\u001b[32m    313\u001b[39m         callback=callback,\n\u001b[32m    314\u001b[39m         log_interval=log_interval,\n\u001b[32m    315\u001b[39m         tb_log_name=tb_log_name,\n\u001b[32m    316\u001b[39m         reset_num_timesteps=reset_num_timesteps,\n\u001b[32m    317\u001b[39m         progress_bar=progress_bar,\n\u001b[32m    318\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28mself\u001b[39m.collect_rollouts(\u001b[38;5;28mself\u001b[39m.env, callback, \u001b[38;5;28mself\u001b[39m.rollout_buffer, n_rollout_steps=\u001b[38;5;28mself\u001b[39m.n_steps)\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[32m    201\u001b[39m     obs_tensor = obs_as_tensor(\u001b[38;5;28mself\u001b[39m._last_obs, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     actions, values, log_probs = \u001b[38;5;28mself\u001b[39m.policy(obs_tensor)\n\u001b[32m    203\u001b[39m actions = actions.cpu().numpy()\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py:653\u001b[39m, in \u001b[36mActorCriticPolicy.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    651\u001b[39m     latent_vf = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_critic(vf_features)\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m values = \u001b[38;5;28mself\u001b[39m.value_net(latent_vf)\n\u001b[32m    654\u001b[39m distribution = \u001b[38;5;28mself\u001b[39m._get_action_dist_from_latent(latent_pi)\n\u001b[32m    655\u001b[39m actions = distribution.get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=400000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preparation for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, we load the delayed data if we have restarted the process (not needed)\n",
    "train = pd.read_csv(DATA_DIR + '/train_data.csv')\n",
    "trade = pd.read_csv(DATA_DIR + '/trade_data.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "trade = trade.set_index(trade.columns[0])\n",
    "trade.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, we load the delayed training data if we have restarted the process (not needed)\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False\n",
    "\n",
    "trained_a2c = A2C.load(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None\n",
    "trained_ddpg = DDPG.load(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo = PPO.load(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3 = TD3.load(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None\n",
    "trained_sac = SAC.load(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trading (data outside the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we now use the trade dataset for trading, which has never been used for training. Additionally, with a similar approach, we could fine-tune the existing saved model.\n",
    "\n",
    "Numerous hyperparameters, such as the learning rate and the total number of training samples, affect the learning process and are usually determined by testing some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dimension = len(trade.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension + len(FUNDAMENTAL_INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 1000,\n",
    "    \"initial_amount\": 100000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"fundamental_indicator_list\": FUNDAMENTAL_INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-2,\n",
    "    \"verbose\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section relates to the use of deep reinforcement learning (DRL) for simulating real-time trading using a previously trained model (in this case, A2C). StockTradingEnv is an environment that simulates the stock market based on historical data, a turbulence threshold (which is used to determine market volatility), and 'VIX' as a risk indicator. After initializing the environment, the get_sb_env() method is used to retrieve the environment and initial observations that are compatible with the Stable Baselines library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the trading environment with the defined DataFrame 'trade', turbulence threshold, and risk indicator 'vix',\n",
    "# along with other environment parameters ('env_kwargs').\n",
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)\n",
    "\n",
    "# Retrieving the environment compatible with Stable Baselines and initial observations.\n",
    "env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DRL agent is then used to predict trading actions and changes in account value based on the trained model. This allows for the evaluation of the model's performance in an out-of-sample scenario, providing insight into how the model might perform in real trading conditions. If the use of the A2C algorithm is selected (as indicated by the variable if_using_a2c), predictions are made; otherwise, the result is set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the DRL agent for prediction using the trained model 'trained_a2c' and the defined trading environment 'e_trade_gym'.\n",
    "# The results are two DataFrames: 'df_account_value_a2c' with account values and 'df_actions_a2c' with the actions taken,\n",
    "# if 'if_using_a2c' is set to True; otherwise, it returns (None, None).\n",
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c, \n",
    "    environment = e_trade_gym) if if_using_a2c else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg, \n",
    "    environment = e_trade_gym) if if_using_ddpg else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo, \n",
    "    environment = e_trade_gym) if if_using_ppo else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_td3, \n",
    "    environment = e_trade_gym) if if_using_td3 else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_sac, \n",
    "    environment = e_trade_gym) if if_using_sac else (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. B&H za usporedbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = YahooDownloader(start_date = TRADE_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = ['AMZN']).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = df_aapl[['date','close']]\n",
    "fst_day = df_aapl['close'].iloc[0]\n",
    "buy_and_hold = pd.merge(df_aapl['date'], df_aapl['close'].div(fst_day).mul(100000), \n",
    "                        how='outer', left_index=True, right_index=True).set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Backtesting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the index of the account value DataFrames for each of the DRL strategies, if selected.\n",
    "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0]) if if_using_a2c else None\n",
    "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0]) if if_using_ddpg else None\n",
    "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0]) if if_using_ppo else None\n",
    "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0]) if if_using_td3 else None\n",
    "df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0]) if if_using_sac else None\n",
    "\n",
    "# Creating an empty DataFrame for results.\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# Merging the results of all DRL strategies into one DataFrame, if selected.\n",
    "if if_using_a2c: result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
    "if if_using_ddpg: result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True, suffixes=('', '_drop'))\n",
    "if if_using_ppo: result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True, suffixes=('', '_drop'))\n",
    "if if_using_td3: result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True, suffixes=('', '_drop'))\n",
    "if if_using_sac: result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True, suffixes=('', '_drop'))\n",
    "\n",
    "# Merging the results of the mean variance optimization and the DJIA index with the main results.\n",
    "result = pd.merge(result, buy_and_hold, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column names based on the selected strategies.\n",
    "col_name = []\n",
    "col_name.append('A2C') if if_using_a2c else None\n",
    "col_name.append('DDPG') if if_using_ddpg else None\n",
    "col_name.append('PPO') if if_using_ppo else None\n",
    "col_name.append('TD3') if if_using_td3 else None\n",
    "col_name.append('SAC') if if_using_sac else None\n",
    "col_name.append('B&H')\n",
    "result.columns = col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the results\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we plot a graph that shows the total portfolio value over time for each strategy and the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the figure size for the plots and drawing the performance charts of the trading strategies.\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the portfolio values, we calculate daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfolio import timeseries\n",
    "import pandas as pd\n",
    "\n",
    "# List of strategies for which you want to generate statistics\n",
    "strategies = ['A2C', 'DDPG', 'PPO', 'TD3', 'SAC', \"B&H\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    column_name = f'{strategy}'\n",
    "    if column_name in result.columns:\n",
    "        # Calculating daily returns from portfolio values\n",
    "        result[f'{strategy}_returns'] = result[column_name].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating returns for the selected strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that the result DataFrame already contains portfolio values for each strategy\n",
    "for strategy in strategies:\n",
    "    # Check if the strategy exists in the DataFrame\n",
    "    if f'{strategy}_returns' in result.columns:\n",
    "        # Extract daily returns for the current strategy\n",
    "        returns = result[f'{strategy}_returns'].dropna()\n",
    "        returns.index = pd.to_datetime(returns.index)\n",
    "\n",
    "        # Calculate performance statistics for the current strategy\n",
    "        perf_stats = timeseries.perf_stats(returns=returns, factor_returns=None, positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "\n",
    "        # Print performance statistics\n",
    "        print(f\"==============Strategy Stats ({strategy})==============\")\n",
    "        print(perf_stats)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Strategy {strategy} was not found in results.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting cumulative returns for each strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "# Setting the figure size for the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterating through each strategy and plotting its cumulative returns\n",
    "for strategy in strategies:\n",
    "    if f'{strategy}_returns' in result.columns:\n",
    "        # Make sure dates are properly parsed\n",
    "        result.index = pd.to_datetime(result.index)\n",
    "        \n",
    "        # Extracting daily returns for the current strategy and converting to decimal format if necessary\n",
    "        returns = result[f'{strategy}_returns'].dropna() + 1  # Adds 1 for using cumprod()\n",
    "        \n",
    "        # Calculating cumulative returns and converting to percentage\n",
    "        cumulative_returns = (returns.cumprod() - 1) * 100  # Convert to percentage\n",
    "        \n",
    "        # Plotting cumulative returns with original colors\n",
    "        plt.plot(cumulative_returns.index, cumulative_returns, label=strategy)\n",
    "\n",
    "# Format y-axis as percentages\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0f}%'.format(y)))\n",
    "plt.ylabel('Cumulative Returns (%)')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "# Format x-axis dates\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "# Make sure dates are in the correct format and rotated\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Adding title and legend\n",
    "#plt.title('Cumulative Returns of DRL Strategies')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Add grid with light gray color\n",
    "plt.grid(True, linestyle='-', alpha=0.2)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results to CSV\n",
    "Needs to be changed for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# For df_account_value_sac\n",
    "with open('results/5. AMZN/amzn_primorl_df_account_value_ppo.pkl', 'wb') as f:\n",
    "    pickle.dump(df_account_value_ppo, f)\n",
    "    \n",
    "# For df_actions_td3\n",
    "with open('results/5. AMZN/amzn_primorl_df_actions_ppo.pkl', 'wb') as f:\n",
    "    pickle.dump(df_actions_ppo, f)\n",
    "\n",
    "# For buy_and_hold\n",
    "with open('results/5. AMZN/amzn_primorl_buy_and_hold.pkl', 'wb') as f:\n",
    "    pickle.dump(buy_and_hold, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
